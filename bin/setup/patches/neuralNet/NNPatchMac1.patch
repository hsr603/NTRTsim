--- /Users/wenrudong/Documents/NTRTsim/env/build/neuralNet/nnImplementationV2/Neural Network v2/neuralNetwork.cpp	2017-09-09 22:49:21.000000000 +0800
+++ /Users/wenrudong/Documents/NTRTsim/env/build/neuralNet/nnImplementationV2/Neural Network v2/neuralNetwork.new.cpp	2017-09-09 22:52:19.000000000 +0800
@@ -26,7 +26,7 @@
 
 	//create neuron lists
 	//----------------------------------------------------------------------
-	
+
 	for ( int i=0; i < nInput; i++ ) inputNeurons[i] = 0;
 
 	//create input bias neuron
@@ -43,23 +43,23 @@
 
 	//create weight lists (include bias neuron weights)
 	//----------------------------------------------------------------------
-	
-	for ( int i=0; i <= nInput; i++ ) 
+
+	for ( int i=0; i <= nInput; i++ )
 	{
 		wInputHidden[i] = new double[nHidden];
-		for ( int j=0; j < nHidden; j++ ) wInputHidden[i][j] = 0;		
+		for ( int j=0; j < nHidden; j++ ) wInputHidden[i][j] = 0;
 	}
 
 	wHiddenOutput = new( double*[nHidden + 1] );
-	for ( int i=0; i <= nHidden; i++ ) 
+	for ( int i=0; i <= nHidden; i++ )
 	{
-		wHiddenOutput[i] = new double[nOutput];			
-		for ( int j=0; j < nOutput; j++ ) wHiddenOutput[i][j] = 0;		
-	}	
-	
+		wHiddenOutput[i] = new double[nOutput];
+		for ( int j=0; j < nOutput; j++ ) wHiddenOutput[i][j] = 0;
+	}
+
 	//initialize weights
 	//----------------------------------------------------------------------
-	initializeWeights();			
+	initializeWeights();
 }
 
 /*******************************************************************
@@ -77,7 +77,7 @@
 	delete[] wInputHidden;
 
 	for (int j=0; j <= nHidden; j++) delete[] wHiddenOutput[j];
-	delete[] wHiddenOutput;	
+	delete[] wHiddenOutput;
 }
 /*******************************************************************
 * Load Neuron Weights
@@ -92,16 +92,16 @@
 	{
 		vector<double> weights;
 		string line = "";
-		
+
 		//read data
 		while ( !inputFile.eof() )
 		{
-			getline(inputFile, line);				
-			
+			getline(inputFile, line);
+
 			//process line
-			if (line.length() > 2 ) 
-			{				
-				//store inputs		
+			if (line.length() > 2 )
+			{
+				//store inputs
 				char* cstr = new char[line.size()+1];
 				char* t;
 				strcpy(cstr, line.c_str());
@@ -109,26 +109,26 @@
 				//tokenise
 				int i = 0;
 				t=strtok (cstr,",");
-				
+
 				while ( t!=NULL )
-				{	
+				{
 					weights.push_back( atof(t) );
-				
+
 					//move token onwards
 					t = strtok(NULL,",");
-					i++;			
+					i++;
 				}
 
 				//free memory
 				delete[] cstr;
 			}
-		}	
-		
+		}
+
 		//check if sufficient weights were loaded
-		if ( weights.size() != ( (nInput + 1) * nHidden + (nHidden +  1) * nOutput ) ) 
+		if ( weights.size() != ( (nInput + 1) * nHidden + (nHidden +  1) * nOutput ) )
 		{
 			cout << endl << "Error - Incorrect number of weights in input file: " << filename << endl;
-			
+
 			//close file
 			inputFile.close();
 
@@ -139,32 +139,32 @@
 			//set weights
 			int pos = 0;
 
-			for ( int i=0; i <= nInput; i++ ) 
+			for ( int i=0; i <= nInput; i++ )
 			{
-				for ( int j=0; j < nHidden; j++ ) 
+				for ( int j=0; j < nHidden; j++ )
 				{
-					wInputHidden[i][j] = weights[pos++];					
+					wInputHidden[i][j] = weights[pos++];
 				}
 			}
-			
-			for ( int i=0; i <= nHidden; i++ ) 
-			{		
-				for ( int j=0; j < nOutput; j++ ) 
+
+			for ( int i=0; i <= nHidden; i++ )
+			{
+				for ( int j=0; j < nOutput; j++ )
 				{
-					wHiddenOutput[i][j] = weights[pos++];						
+					wHiddenOutput[i][j] = weights[pos++];
 				}
-			}	
+			}
 
 			//print success
 			cout << endl << "Neuron weights loaded successfuly from '" << filename << "'" << endl;
 
 			//close file
 			inputFile.close();
-			
+
 			return true;
-		}		
+		}
 	}
-	else 
+	else
 	{
 		cout << endl << "Error - Weight input file '" << filename << "' could not be opened: " << endl;
 		return false;
@@ -181,22 +181,22 @@
 
 	if ( outputFile.is_open() )
 	{
-		outputFile.precision(50);		
+		outputFile.precision(50);
 
 		//output weights
-		for ( int i=0; i <= nInput; i++ ) 
+		for ( int i=0; i <= nInput; i++ )
 		{
-			for ( int j=0; j < nHidden; j++ ) 
+			for ( int j=0; j < nHidden; j++ )
 			{
-				outputFile << wInputHidden[i][j] << ",";				
+				outputFile << wInputHidden[i][j] << ",";
 			}
 		}
-		
-		for ( int i=0; i <= nHidden; i++ ) 
-		{		
-			for ( int j=0; j < nOutput; j++ ) 
+
+		for ( int i=0; i <= nHidden; i++ )
+		{
+			for ( int j=0; j < nOutput; j++ )
 			{
-				outputFile << wHiddenOutput[i][j];					
+				outputFile << wHiddenOutput[i][j];
 				if ( i * nOutput + j + 1 != (nHidden + 1) * nOutput ) outputFile << ",";
 			}
 		}
@@ -206,10 +206,10 @@
 
 		//close file
 		outputFile.close();
-		
+
 		return true;
 	}
-	else 
+	else
 	{
 		cout << endl << "Error - Weight output file '" << filename << "' could not be created: " << endl;
 		return false;
@@ -235,28 +235,28 @@
 double neuralNetwork::getSetAccuracy( std::vector<dataEntry*>& set )
 {
 	double incorrectResults = 0;
-		
+
 	//for every training input array
 	for ( int tp = 0; tp < (int) set.size(); tp++)
-	{						
+	{
 		//feed inputs through network and backpropagate errors
 		feedForward( set[tp]->pattern );
-		
+
 		//correct pattern flag
 		bool correctResult = true;
 
 		//check all outputs against desired output values
 		for ( int k = 0; k < nOutput; k++ )
-		{					
+		{
 			//set flag to false if desired and output differ
 			if ( clampOutput(outputNeurons[k]) != set[tp]->target[k] ) correctResult = false;
 		}
-		
+
 		//inc training error for a incorrect result
-		if ( !correctResult ) incorrectResults++;	
-		
+		if ( !correctResult ) incorrectResults++;
+
 	}//end for
-	
+
 	//calculate error and return as percentage
 	return 100 - (incorrectResults/set.size() * 100);
 }
@@ -266,22 +266,22 @@
 double neuralNetwork::getSetMSE( std::vector<dataEntry*>& set )
 {
 	double mse = 0;
-		
+
 	//for every training input array
 	for ( int tp = 0; tp < (int) set.size(); tp++)
-	{						
+	{
 		//feed inputs through network and backpropagate errors
 		feedForward( set[tp]->pattern );
-		
+
 		//check all outputs against desired output values
 		for ( int k = 0; k < nOutput; k++ )
-		{					
+		{
 			//sum all the MSEs together
 			mse += pow((outputNeurons[k] - set[tp]->target[k]), 2);
-		}		
-		
+		}
+
 	}//end for
-	
+
 	//calculate error and return as percentage
 	return mse/(nOutput * set.size());
 }
@@ -293,24 +293,24 @@
 	//set range
 	double rH = 1/sqrt( (double) nInput);
 	double rO = 1/sqrt( (double) nHidden);
-	
-	//set weights between input and hidden 		
+
+	//set weights between input and hidden
 	//--------------------------------------------------------------------------------------------------------
 	for(int i = 0; i <= nInput; i++)
-	{		
-		for(int j = 0; j < nHidden; j++) 
+	{
+		for(int j = 0; j < nHidden; j++)
 		{
 			//set weights to random values
 			wInputHidden[i][j] = ( ( (double)(rand()%100)+1)/100  * 2 * rH ) - rH;
 			cout<<"weight set: "<<wInputHidden[i][j]<<endl;
 		}
 	}
-	
+
 	//set weights between input and hidden
 	//--------------------------------------------------------------------------------------------------------
 	for(int i = 0; i <= nHidden; i++)
-	{		
-		for(int j = 0; j < nOutput; j++) 
+	{
+		for(int j = 0; j < nOutput; j++)
 		{
 			//set weights to random values
 			wHiddenOutput[i][j] = ( ( (double)(rand()%100)+1)/100 * 2 * rO ) - rO;
@@ -324,7 +324,7 @@
 {
 	//sigmoid function
 	return 1/(1+exp(-x));
-}	
+}
 /*******************************************************************
 * Output Clamping
 ********************************************************************/
@@ -341,31 +341,31 @@
 {
 	//set input neurons to input values
 	for(int i = 0; i < nInput; i++) inputNeurons[i] = pattern[i];
-	
+
 	//Calculate Hidden Layer values - include bias neuron
 	//--------------------------------------------------------------------------------------------------------
 	for(int j=0; j < nHidden; j++)
 	{
 		//clear value
-		hiddenNeurons[j] = 0;				
-		
+		hiddenNeurons[j] = 0;
+
 		//get weighted sum of pattern and bias neuron
 		for( int i=0; i <= nInput; i++ ) hiddenNeurons[j] += inputNeurons[i] * wInputHidden[i][j];
-		
+
 		//set to result of sigmoid
-		hiddenNeurons[j] = activationFunction( hiddenNeurons[j] );			
+		hiddenNeurons[j] = activationFunction( hiddenNeurons[j] );
 	}
-	
+
 	//Calculating Output Layer values - include bias neuron
 	//--------------------------------------------------------------------------------------------------------
 	for(int k=0; k < nOutput; k++)
 	{
 		//clear value
-		outputNeurons[k] = 0;				
-		
+		outputNeurons[k] = 0;
+
 		//get weighted sum of pattern and bias neuron
 		for( int j=0; j <= nHidden; j++ ) outputNeurons[k] += hiddenNeurons[j] * wHiddenOutput[j][k];
-		
+
 		//set to result of sigmoid
 		outputNeurons[k] = activationFunction( outputNeurons[k] );
 	}
@@ -404,9 +404,9 @@
 	}
 }
 
-void neuralNetwork::combineWeights(neuralNetwork * nn1, neuralNetwork * nn2, std::tr1::ranlux64_base_01 *eng)
+void neuralNetwork::combineWeights(neuralNetwork * nn1, neuralNetwork * nn2, std::ranlux48_base *eng)
 {
-    std::tr1::uniform_real<double> unif(0, 1);
+    std::uniform_real_distribution<double> unif(0, 1);
     //delete weight storage
     for (int i=0; i <= nInput; i++) delete[] wInputHidden[i];
     delete[] wInputHidden;
@@ -418,7 +418,7 @@
     this->nInput=nn1->nInput;
     this->nHidden=nn1->nHidden;
     this->nOutput=nn1->nOutput;
-    
+
     assert(nn1->nInput == nn2->nInput && nn1->nHidden == nn2->nHidden && nn1->nOutput == nn2->nOutput);
 
     //reallocate and copy weights
@@ -426,7 +426,7 @@
     for ( int i=0; i <= nInput; i++ )
     {
         wInputHidden[i] = new double[nHidden];
-        for ( int j=0; j < nHidden; j++ ) 
+        for ( int j=0; j < nHidden; j++ )
         {
             if(unif(*eng)  > 0.5)
                 wInputHidden[i][j] = nn1->wInputHidden[i][j];
@@ -449,9 +449,9 @@
     }
 }
 
-void neuralNetwork::mutate(std::tr1::ranlux64_base_01 *eng)
+void neuralNetwork::mutate(std::ranlux48_base *eng)
 {
-	std::tr1::uniform_real<double> unif(0, 1);
+	std::uniform_real_distribution<double> unif(0, 1);
 	for ( int i=0; i <= nInput; i++ )
 	{
 		for ( int j=0; j < nHidden; j++ )
@@ -460,7 +460,7 @@
 				continue;
 			double range=10.0; //range of the variable
 			double dev = 10.0 * range / 100.0;  //10% of the range
-			std::tr1::normal_distribution<double> normal(0, dev);
+			std::normal_distribution<double> normal(0, dev);
 			double mutAmount = normal(*eng);
 			cout<<"mutated weight from: "<<wInputHidden[i][j]<<" with"<<mutAmount<<endl;
 			wInputHidden[i][j]+=mutAmount;
@@ -475,7 +475,7 @@
 				continue;
 			double range=10.0; //range of the variable
 			double dev = 10.0 * range / 100.0;  //10% of the range
-			std::tr1::normal_distribution<double> normal(0, dev);
+			std::normal_distribution<double> normal(0, dev);
 			double mutAmount = normal(*eng);
 			cout<<"mutated weight from: "<<wHiddenOutput[i][j]<<" with "<<mutAmount<<endl;
 			wHiddenOutput[i][j] = mutAmount;
